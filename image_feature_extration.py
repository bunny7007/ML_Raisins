# -*- coding: utf-8 -*-
"""ML

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19expohSE6itBIkuIIEW2AhXSwZJV8JXV
"""

# Conversion of image
import cv2
from google.colab.patches import cv2_imshow
image = cv2.imread('/content/sample_data/WhatsApp Image 2023-11-13 at 14.42.28.jpeg')
gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
_, binary_image = cv2.threshold(gray_image, 127, 255, cv2.THRESH_BINARY)

inverted_image = cv2.bitwise_not(binary_image)
cv2_imshow(image)
cv2_imshow(gray_image)
cv2_imshow(binary_image)
cv2_imshow(contour_img)
cv2_imshow(inverted_image)

# Assuming 'binary_image.png' is your original binary image
binary_image = cv2.imread('/content/Unknown.png', cv2.IMREAD_GRAYSCALE)
inverted_binary_image = cv2.bitwise_not(binary_image)
cv2_imshow(inverted_binary_image)
import cv2
import numpy as np
from google.colab.patches import cv2_imshow

# Read the inverted binary image
inverted_binary_image = cv2.imread('/content/Unknown-2.png', cv2.IMREAD_GRAYSCALE)

# Find contours
contours, _ = cv2.findContours(inverted_binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# Draw the contours on a blank image
contour_image = np.zeros_like(inverted_binary_image, dtype=np.uint8)
cv2.drawContours(contour_image, contours, -1, (255, 255, 255), 2)

# Display the original inverted binary image and the image with contours
cv2_imshow(inverted_binary_image)
cv2_imshow(contour_image)
cv2.waitKey(0)
cv2.destroyAllWindows()

pip install mahotas

import cv2
import numpy as np
import mahotas.features as ft


# Read the inverted binary image
inverted_binary_image = cv2.imread('/content/WhatsApp Image 2023-11-14 at 21.50.06.jpeg', cv2.IMREAD_GRAYSCALE)

# Find contours
contours, _ = cv2.findContours(inverted_binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

some_threshold = 1000

# Filter contours based on your criteria (you may need to adjust this part)
filtered_contours = [contour for contour in contours if cv2.contourArea(contour) > some_threshold]

# Initialize an empty list to store extracted features
extracted_features = []

# Loop through the filtered contours and calculate features
for contour in filtered_contours:
    # Calculate features
    area = cv2.contourArea(contour)
    perimeter = cv2.arcLength(contour, True)
    _, (major_axis, minor_axis), _ = cv2.fitEllipse(contour)
    eccentricity = np.sqrt(1 - (minor_axis / major_axis) )
    extent = area / (major_axis * minor_axis)
    convex_area = cv2.contourArea(cv2.convexHull(contour))

    # Append the features to the list
    extracted_features.append({
        'Area': area,
        'Perimeter': perimeter,
        'MajorAxisLength': major_axis,
        'MinorAxisLength': minor_axis,
        'Eccentricity': eccentricity,
        'Extent': extent,
        'ConvexArea': convex_area
    })

# Display the extracted features
for i, features in enumerate(extracted_features, 1):
    print(f"Raisin {i} Features: {features}")

import cv2
import numpy as np

# Read the inverted binary image
inverted_binary_image = cv2.imread('/content/WhatsApp Image 2023-11-14 at 21.50.06.jpeg', cv2.IMREAD_GRAYSCALE)

# Find contours
contours, _ = cv2.findContours(inverted_binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

some_threshold = 1000

# Filter contours based on your criteria (you may need to adjust this part)
filtered_contours = [contour for contour in contours if cv2.contourArea(contour) > some_threshold]

# Initialize an empty list to store extracted features
extracted_features = []

# Loop through the filtered contours and calculate features
for contour in filtered_contours:
    # Calculate features
    area = cv2.contourArea(contour)
    perimeter = cv2.arcLength(contour, True)

    # Fit an ellipse to the contour
    ellipse = cv2.fitEllipse(contour)
    (center, axes, angle) = ellipse
    major_axis, minor_axis = axes

    # Check if both major and minor axes are greater than zero
    if major_axis > 0 and minor_axis > 0:
        eccentricity = np.sqrt(1 - (minor_axis / major_axis) ** 2)
    else:
        eccentricity = 0  # Set a default value or handle it accordingly

    extent = area / (major_axis * minor_axis)
    convex_area = cv2.contourArea(cv2.convexHull(contour))

    # Append the features to the list
    extracted_features.append({
        'Area': area,
        'Perimeter': perimeter,
        'MajorAxisLength': major_axis,
        'MinorAxisLength': minor_axis,
        'Eccentricity': eccentricity,
        'Extent': extent,
        'ConvexArea': convex_area
    })

# Display the extracted features
for i, features in enumerate(extracted_features, 1):
    print(f"Raisin {i} Features: {features}")



import cv2
import numpy as np
from google.colab.patches import cv2_imshow

# Read the inverted binary image
inverted_binary_image = cv2.imread('/content/123.jpg', cv2.IMREAD_GRAYSCALE)

# Check if the image is loaded successfully
if inverted_binary_image is None:
    print("Error: Unable to load the image.")
else:
    # Apply adaptive thresholding
    adaptive_threshold = cv2.adaptiveThreshold(
        inverted_binary_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2
    )

    # Find contours
    contours, _ = cv2.findContours(adaptive_threshold, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # Initialize an empty list to store extracted features
    extracted_features = []

    # Loop through the contours and calculate features
    for i, contour in enumerate(contours, 1):
        # Ensure that the contour has enough points for fitting an ellipse
        if len(contour) >= 5:
            # Calculate features
            area = cv2.contourArea(contour)
            perimeter = cv2.arcLength(contour, True)
            _, (major_axis, minor_axis), _ = cv2.fitEllipse(contour)
            eccentricity = np.sqrt(1 - (minor_axis / major_axis) ** 2)
            extent = area / (major_axis * minor_axis)
            convex_area = cv2.contourArea(cv2.convexHull(contour))

            # Append the features to the list
            extracted_features.append({
                'Area': area,
                'Perimeter': perimeter,
                'MajorAxisLength': major_axis,
                'MinorAxisLength': minor_axis,
                'Eccentricity': eccentricity,
                'Extent': extent,
                'ConvexArea': convex_area
            })
        else:
            print(f"Contour {i} does not have enough points for fitting an ellipse.")

    # Display the number of contours found
    print(f"Number of contours: {len(contours)}")

    # Display the original image, binary threshold image, and the contours
    cv2_imshow(inverted_binary_image)
    cv2_imshow(adaptive_threshold)
    cv2.drawContours(inverted_binary_image, contours, -1, (0, 255, 0), 2)
    cv2_imshow(inverted_binary_image)

    # Display the extracted features
    for i, features in enumerate(extracted_features, 1):
        print(f"Raisin {i} Features: {features}")

    cv2.waitKey(0)
    cv2.destroyAllWindows()

#Contour of an image
import cv2
import numpy as np

# Load the color image
image = cv2.imread('/content/sample_data/WhatsApp Image 2023-11-13 at 14.42.28.jpeg')
# Define the color range for the black raisin (adjust this based on your image)
lower_black = np.array([0, 0, 0])  # Lower bound for black color
upper_black = np.array([90, 190, 240])  # Upper bound for black color

# Create a binary mask for the raisin
mask = cv2.inRange(image, lower_black, upper_black)


# Find contours in the binary mask
contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# Filter the contours, e.g., choose the largest contour
raisin_contour = max(contours, key=cv2.contourArea)

# Draw the raisin contour on the original image
cv2.drawContours(image, [raisin_contour], -1, (0, 255, 0), 2)

# Display the image with the highlighted raisin
cv2_imshow(image)
cv2.waitKey(0)
cv2.destroyAllWindows()

import cv2
import numpy as np

# Load the color image
image = cv2.imread('/content/123.jpg')

# Check if the image is loaded successfully
if image is None:
    print("Error: Unable to load the image.")
else:
    # Define the color range for the black raisin (adjust this based on your image)
    lower_black = np.array([0, 0, 0])  # Lower bound for black color
    upper_black = np.array([150, 150, 150])  # Upper bound for black color

    # Create a binary mask for the raisin
    mask = cv2.inRange(image, lower_black, upper_black)

    # Find contours in the binary mask
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # Filter the contours, e.g., choose the largest contour
    raisin_contour = max(contours, key=cv2.contourArea)


    # Calculate contour features for each contour
    for contour in contours:
        area = cv2.contourArea(contour)
        perimeter = cv2.arcLength(contour, True)
        (x, y), (major_axis_length, minor_axis_length)
        angle = cv2.fitEllipse(contour)
        eccentricity = np.sqrt(1 - (minor_axis_length / major_axis_length) ** 2)
        extent = area / (image.shape[0] * image.shape[1])
        convex_hull = cv2.convexHull(contour)
        convex_area = cv2.contourArea(convex_hull)

    # Rest of your code...
    for contour in contours:
        # Calculate area, perimeter, fit ellipse, and other features...
        print("Area:", area)
        print("Perimeter:", perimeter)
        print("Major Axis Length:", major_axis_length)
        print("Minor Axis Length:", minor_axis_length)
        print("Eccentricity:", eccentricity)
        print("Extent:", extent)
        print("Convex Area:", convex_area)
        print("-" * 30)

    # Draw the raisin contour on the original image
    cv2.drawContours(image, [raisin_contour], -1, (0, 255, 0), 2)

    # Display the image with the highlighted raisin
    cv2.imshow("Highlighted Raisin", image)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

import cv2
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator

def preprocess_images(images, labels, target_size=(224, 224), augment=False):
    """
    Preprocess images for a machine learning model.

    Parameters:
    - images: Numpy array of images.
    - labels: Numpy array of corresponding labels.
    - target_size: Tuple specifying the target size for resizing images.
    - augment: Boolean indicating whether to apply data augmentation.

    Returns:
    - Preprocessed images and labels.

    """
    image = cv2.imread('/content/sample_data/WhatsApp Image 2023-11-13 at 15.03.21.jpeg')
    # Resize images to a consistent size
    resized_images = [cv2.resize(image, target_size) for image in images]

    # Convert the list of images to a numpy array
    resized_images = np.array(resized_images)

    # Normalize pixel values to the range [0, 1]
    normalized_images = resized_images / 255.0

    if augment:
        # Data augmentation using TensorFlow's ImageDataGenerator
        datagen = ImageDataGenerator(
            rotation_range=20,
            width_shift_range=0.2,
            height_shift_range=0.2,
            shear_range=0.2,
            zoom_range=0.2,
            horizontal_flip=True,
            fill_mode='nearest'
        )

        # Fit the generator on the data
        datagen.fit(normalized_images)

        # Augment the images
        augmented_images = []
        augmented_labels = []
        for images_batch, labels_batch in datagen.flow(resized_images, labels, batch_size=len(resized_images)):
            augmented_images = images_batch
            augmented_labels = labels_batch
            break

        return augmented_images, augmented_labels

    return normalized_images, labels
    from sklearn.model_selection import train_test_split

# Assuming you have features (X) and labels (y)
# X can be your preprocessed images, and y can be the corresponding labels

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split( labels, test_size=0.2, random_state=42)

# Print the shapes of the resulting sets (optional)
print("Training set shape:", X_train.shape, y_train.shape)
print("Testing set shape:", X_test.shape, y_test.shape)

import cv2
import numpy as np

def classify_raisin_color(image_path):
    # Read the image
    img = cv2.imread(image_path)

    # Convert the image to grayscale
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # Calculate the average color intensity
    avg_color = np.mean(gray)

    # Calculate the mean color values
    mean_color = np.mean(img, axis=(0, 1))

    # Define a threshold value
    threshold = 100  # Adjust this value based on your preferences and testing

    # Classify based on the mean color values
    if mean_color[0] > 151 and mean_color[1] > 151 and mean_color[2] < 151:
        return "Black Raisin"
    elif mean_color[0] < 150 and mean_color[1] < 150 and mean_color[2] < 150:
        return "Yell0w Raisin"
    else:
        return "Unclassified"

# Test the classification with an example image
image_path = "/content/sample_data/WhatsApp Image 2023-11-13 at 14.42.28.jpeg"  # Replace with the path to your image
classification_result = classify_raisin_color(image_path)
print(classification_result)

import cv2
import numpy as np
from google.colab.patches import cv2_imshow

def classify_raisin_color(contour_image):
    # Calculate the mean color values from the contour image
    mean_color = np.mean(contour_image, axis=(0, 1))

    # Define your classification criteria based on the mean color values
    if mean_color[0] > 150 and mean_color[1] > 150 and mean_color[2] < 100:
        return "Yellow Raisin"
    elif mean_color[0] < 100 and mean_color[1] < 100 and mean_color[2] < 100:
        return "Black Raisin"
    else:
        return "Unclassified"

# Read the original image
image_path = "/content/123.jpg"  # Replace with the path to your image
original_image = cv2.imread(image_path)

# Convert the image to grayscale
gray = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)

# Apply a GaussianBlur to smooth the image and reduce noise
blurred = cv2.GaussianBlur(gray, (5, 5), 0)

# Use adaptive thresholding to create a binary image
_, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

# Find contours in the binary image
contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# Create a black image with the same size as the original image
contour_image = np.zeros_like(original_image)

# Draw contours on the black image
cv2.drawContours(contour_image, contours, -1, (255, 255, 255), thickness=cv2.FILLED)

# Display the contour image
cv2_imshow(contour_image)
cv2.waitKey(0)
cv2.destroyAllWindows()

# Classify the raisin based on the mean color values from the contour image
classification_result = classify_raisin_color(contour_image)
print("Classification Result:", classification_result)

import cv2
import numpy as np

def classify_raisin_color(contour_image):
    # Calculate the mean color values from the contour image
    mean_color = np.mean(contour_image, axis=(0, 1))

    # Define your classification criteria based on the mean color values
    if mean_color[0] > 150 and mean_color[1] > 150 and mean_color[2] < 100:
        return "Dark Raisin"
    else:
        return "Light Raisin"

# Assuming you have a function to get the contour of the raisin
def get_raisin_contour(image_path):
    # Read the image
    img = cv2.imread(image_path)

    # Convert the image to grayscale
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # Apply a GaussianBlur to smooth the image and reduce noise
    blurred = cv2.GaussianBlur(gray, (5, 5), 0)

    # Use adaptive thresholding to create a binary image
    _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

    # Find contours in the binary image
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # Create a black image with the same size as the original image
    contour_image = np.zeros_like(img)

    # Draw contours on the black image
    cv2.drawContours(contour_image, contours, -1, (255, 255, 255), thickness=cv2.FILLED)

    return contour_image

# Test the classification with an example image
image_path = "/content/sample_data/WhatsApp Image 2023-11-13 at 14.43.08.jpeg"  # Replace with the path to your image

# Get the raisin contour
contour_image = get_raisin_contour(image_path)

# Classify the raisin based on the mean color values from the contour image
classification_result = classify_raisin_color(contour_image)
print(classification_result)







import cv2
import numpy as np

def classify_raisin_color(contour_image):
    # Calculate the mean color values from the contour image
    mean_color = np.mean(contour_image, axis=(0, 1))

    # Define your classification criteria based on the mean color values
    if mean_color[0] > 150 and mean_color[1] > 150 and mean_color[2] < 100:
        return "Light Raisin"
    else:
        return "Dark Raisin"

# Assuming you have a function to get the contour of the raisin
def get_raisin_contour(image_path):
    # Read the image
    img = cv2.imread(image_path)

    # Convert the image to grayscale
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # Apply a GaussianBlur to smooth the image and reduce noise
    blurred = cv2.GaussianBlur(gray, (5, 5), 0)

    # Use adaptive thresholding to create a binary image
    _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

    # Find contours in the binary image
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # Create a black image with the same size as the original image
    contour_image = np.zeros_like(img)

    # Draw contours on the black image
    cv2.drawContours(contour_image, contours, -1, (255, 255, 255), thickness=cv2.FILLED)

    return contour_image

# Test the classification with an example image
image_path = "/content/1234.jpg"  # Replace with the path to your image

# Get the raisin contour
contour_image = get_raisin_contour(image_path)

# Classify the raisin based on the mean color values from the contour image
classification_result = classify_raisin_color(contour_image)
print(classification_result)

import matplotlib.pyplot as plt

# Display the original and contour images
plt.subplot(1, 2, 1)
plt.imshow(cv2.cvtColor(cv2.imread('/content/sample_data/WhatsApp Image 2023-11-13 at 14.43.08.jpeg'), cv2.COLOR_BGR2RGB))
plt.title("Original Image")

plt.subplot(1, 2, 2)
plt.imshow(contour_image, cmap='gray')
plt.title("Contour Image")

plt.show()

"""GITHUB CODE"""

##Importing libraries##
import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
from sklearn.utils import shuffle
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import svm
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import make_scorer, accuracy_score
from sklearn import metrics
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB

raisin = pd.read_csv('/content/Raisin_Dataset.xlsx')
raisin.head()

import numpy as nm
import matplotlib.pyplot as mtp
import pandas as pd



data_set= pd.imread_csv('/content/Raisin_Dataset.xlsx')





# Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# SVM libraries
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC, SVR
from sklearn.datasets import load_digits
from sklearn.metrics import classification_report

!pip install openpyxl

# Read data set
data = pd.read_excel('Raisin_Dataset.xlsx', sheet_name = 'Raisin_Dataset')
data

data.info()

print(data)

data['Class'] = data['Class'].str.replace('Kecimen', '0')
data['Class'] = data['Class'].str.replace('Besni', '1')

cols = data.columns
data[cols] = data[cols].apply(pd.to_numeric, errors='coerce')

# Data frame structure
data.info()

print(data)

import pandas as pd

# Assuming 'data' is your DataFrame
data['Class'] = data['Class'].replace('Kecimen', 0)
data['Class'] = data['Class'].replace('Besini', 1)

# Alternatively, you can use a dictionary for replacement
class_mapping = {'Kecimen': 0, 'Besini': 1}
data['Class'] = data['Class'].replace(class_mapping)

# Convert the 'Class' column to numeric (if it's not already)
data['Class'] = pd.to_numeric(data['Class'], errors='coerce')

# Check the data types
print(data.dtypes)

# Classes are distrubuted evenly
data['Class'].value_counts()

X = data
X = X.drop(['Class'], axis = 1)
Y = data.Class # class

# Parameters for classificator - polynomial kernel
degree = [1,2,3,4,5,6,7,8,9]
degree = np.array(degree)

# Tables to save accuracy - polynomial kernel
Accuracy_CV_poly = np.zeros((10,1))
Accuracy_poly = np.zeros(len(degree))

# SVM - polynomial kernel
for i in range(0, len(degree)): # degree
    for k in range(1, 10): # Crossvalidation
        # Split data into test and train sets
        X_train_poly, X_test_poly, Y_train_poly, Y_test_poly = train_test_split(X, Y, test_size = 0.10) # 10 times cs so test_size is 10% of data set

        # Standarization
        sc = StandardScaler()
        sc.fit(X_train_poly)
        X_train_poly = sc.transform(X_train_poly)
        X_test_poly = sc.transform(X_test_poly)
        X_train_poly = pd.DataFrame(X_train_poly)
        X_test_poly = pd.DataFrame(X_test_poly)

        # Model
        svclassifier = SVC(kernel='poly', degree = degree[i])
        svclassifier.fit(X_train_poly, Y_train_poly)

        y_pred_poly = svclassifier.predict(X_test_poly)

        # Accuracy - how many values from y_pred are equal to Y_test
        Accuracy_CV_poly[k] = sum(y_pred_poly == Y_test_poly)/len(Y_test_poly)

    Accuracy_poly[i] = np.mean(Accuracy_CV_poly) # rows - gamma, columns - C

Accuracy_poly # display accuracy table

# polynomial kernel accuracy plot
# axis X - degree
plt.plot(Accuracy_poly)
plt.xlabel('degree')
plt.ylabel('Accuracy')
#plt.xticks(np.arange(len(degree)), degree) # correct axis X ticks

# Parameters for classificator - rbf kernel
gamma = [0.0005, 0.005, 0.01, 0.05, 0.2, 0.8, 1.5, 2.5, 5, 10, 20, 50, 100]
gamma = np.array(gamma)
C = [1, 10, 100, 1000, 10000, 100000]
C = np.array(C)

# Tables to save accuracy - rbf kernel
Accuracy_CV = np.zeros((10,1))
Accuracy = np.zeros((len(gamma), len(C)))

# SVM - rbf kernel
for i in range(0, len(C)): # C
    for j in range(0, len(gamma)): # gamma
        for k in range(1, 10): # Crossvalidation
            # Split data into test and train sets
            X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.10) # 10 times cs so test_size is 10% of data set

            # Standarization
            sc = StandardScaler()
            sc.fit(X_train)
            X_train = sc.transform(X_train)
            X_test = sc.transform(X_test)
            X_train = pd.DataFrame(X_train)
            X_test = pd.DataFrame(X_test)

            # Model
            svclassifier = SVC(kernel = 'rbf', C = C[i], gamma = gamma[j])

            svclassifier.fit(X_train, Y_train)
            y_pred = svclassifier.predict(X_test)

            # Accuracy - how many values from y_pred are equal to Y_test
            Accuracy_CV[k] = sum(y_pred == Y_test)/len(Y_test)

        Accuracy[j,i] = np.mean(Accuracy_CV) # rows - gamma, columns - C

Accuracy # display accuracy table

# rbf kernel accuracy plot
# axis X - gamma, different lines - C
for p in range(0, len(C)): # number of lines = number of C values
    plt.plot(Accuracy[:,p], label = C[p]) # every line plotted separately in order to have a name
plt.xlabel('Gamma')
plt.ylabel('Accuracy')
plt.xticks(np.arange(len(gamma)), gamma) # correct axis X ticks
plt.legend(title = 'C')

C = [1, 10, 100, 1000, 10000, 100000]
C = np.array(C)

k_range = 10
Accuracy_CV2 = np.zeros((len(C),2))
Accuracy_rbf2 = np.zeros((k_range,2))
# SVM - rbf kernel - k is outside loop
for k in range(0, k_range):
    # Split data into test and train sets
    X_train_rbf2, X_test_rbf2, Y_train_rbf2, Y_test_rbf2 = train_test_split(X, Y, test_size = k_range/100) # 10 times cs so test_size is 10% of data set

    # Standarization
    sc = StandardScaler()
    sc.fit(X_train_rbf2)
    X_train_rbf2 = sc.transform(X_train_rbf2)
    X_test_rbf2 = sc.transform(X_test_rbf2)
    X_train_rbf2 = pd.DataFrame(X_train_rbf2)
    X_test_rbf2 = pd.DataFrame(X_test_rbf2)

    for i in range(0, len(C)):
        # Model
        svclassifier = SVC(kernel = 'rbf', C = C[i], gamma = 0.05) # gamma constant
        svclassifier.fit(X_train_rbf2, Y_train_rbf2)
        y_pred_rbf2 = svclassifier.predict(X_test_rbf2)

        # Accuracy
        Accuracy_CV2[i,0] = sum(y_pred_rbf2 == Y_test_rbf2)/len(Y_test_rbf2)
        Accuracy_CV2[i,1] = C[i]

    Accuracy_rbf2[k,0] = np.max(Accuracy_CV2[:,0]) # best accuracy
    id_max = np.argmax(Accuracy_CV2[:,0], axis=0)
    Accuracy_rbf2[k,1] = Accuracy_CV2[id_max,1] # C for best acuracy

print("Accuracy table:\n", Accuracy_rbf2) # display accuracy table

# Average accuracy
Accuracy_mean = np.mean(Accuracy_rbf2[:,0])
print("Mean accuracy =", Accuracy_mean)

# Accuracy closest to average accuracy
Accuarcy_roznica = Accuracy_rbf2[:,0] - Accuracy_mean
Accuarcy_roznica = abs(Accuarcy_roznica)
id_min = np.argmin(Accuarcy_roznica, axis=0)
avg_acc_C = Accuracy_rbf2[id_min, 1]
avg_acc = Accuracy_rbf2[id_min, 0]
print("C for value closest to average accuracy:",avg_acc_C)

# Split
X_train_last, X_test_last, Y_train_last, Y_test_last = train_test_split(X, Y, test_size = 0.10) # 10 times cs so test_size is 10% of data set

# Standarization
sc = StandardScaler()
sc.fit(X_train_last)
X_train_last = sc.transform(X_train_last)
X_test_last = sc.transform(X_test_last)
X_train_last = pd.DataFrame(X_train_last)
X_test_last = pd.DataFrame(X_test_last)

# ModelC[i]
svclassifier = SVC(kernel = 'rbf', C = avg_acc_C, gamma = 0.05)

svclassifier.fit(X_train_last, Y_train_last)
y_pred_last = svclassifier.predict(X_test_last)

# Accuracy - how many values from y_pred are equal to Y_test
Accuracy_CV_last = sum(y_pred_last == Y_test_last)/len(Y_test_last)
print("Accuracy:", Accuracy_CV_last)

print(classification_report(Y_test_last, y_pred_last, target_names=['Besni', 'Kecimen']))

import numpy as np

# Create a synthetic sample with the same number of features as your original dataset
# Replace this with actual values that are representative of your dataset
sample_input = np.array([43725,301.3222176,186.9506295,0,45021,0.697068248,818.873]).reshape(1, -1)

# Standarization
sc = StandardScaler()
sc.fit(X)  # Assuming X is your original dataset
sample_input = sc.transform(sample_input)

# Make predictions
sample_prediction = svclassifier.predict(sample_input)

# Print the predicted class
print("Predicted Class:", sample_prediction[0])











